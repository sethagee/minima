---
layout: post
usemathjax: true
title: "The Langevin Algorithm for Linear Models part 1"
---

## Normal linear model
For a normal linear model, we have for a response variable ($$ m \times 1 $$ vector) $$ y $$, regression coefficients/parameters ($$ (n+1) \times 1 $$ vector)  $$ \theta $$, and design matrix ($$ m \times (n+1) $$ matrix) $$ X $$ 

$$ y = X\theta + \varepsilon, $$

with $$ \varepsilon \sim N(0, \sigma^2 I_m) $$.

In this model, the least-square estimate has the property

$$ \hat{\theta} = (X^TX)^{-1}X^Ty = \theta + (X^TX)^{-1}X^T\varepsilon \sim N(\theta, \sigma(X^TX)^{-1}). $$

That is, the estimate is normal with center $$ \theta $$.

The cost function 

$$ J(\theta) = (X\theta - y)^T (X\theta - y) $$

has gradient 

$$ \nabla J(\theta) = 2X^T(X\theta - y) $$

which can be used in the gradient descent algorithm, where in each step

$$ \theta_{t+1} = \theta_t - \alpha \nabla J(\theta) = \theta_t - 2\alpha X^T(X\theta - y), $$

for some learning rate or step size $$ \alpha $$. 

Observe that 

$$ J(\theta) = y^T (I_m - X(X^TX)^{-1}X^T) y + (\theta-\hat{\theta})^TX^TX(\theta - \hat{\theta}) $$

where the minimum value (at $$ \hat{\theta} = \theta $$) is $$ y^T (I_m - X(X^TX)^{-1}X^T) y $$. In particular, we want to minimize the quadratic form 

$$ (\theta-\hat{\theta})^TX^TX(\theta - \hat{\theta}). $$

Since $$ X^T X $$ positive semi-definite, one can think of the graph of this function (in some way) as a parabola. The axis length of this parabola, as it turns out is proportional to the eigenvalues of the matrix $$ X^T X $$. So, for a matrix having eigenvalues of $$ X^TX $$ very small or very large, this can cause great disparities in axis lengths, making the choice for $$ \alpha $$ very sensitive. Below is an example of such a data set.


```python
# Packages
import numpy as np
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot as plt

%matplotlib inline
```


```python
# This data will cause problems for gradient descent 
X = np.array([
    [1, 1, 235],
    [1, 5.2, 436],
    [1, 4, 301],
    [1, 7.4, 1034],
    [1, 9, 1679]
])

y = np.array([
    1.1,
    6.3,
    4.4,
    8,
    11.3
])
```

According to sklearn, the coefficients should be approximately $$ \theta = [-0.1074, 1.0677, 0.0009] $$. The eigenvalues of $$ X^TX $$ are 


```python
egv = np.zeros(X.shape[1])
egv = np.linalg.eigvals(X.T @ X)

print(egv)
```

    [4.22428807e+06 8.45069206e-01 1.48832075e+01]
    

In this case, one eigenvalue is much larger than the others, causing the issue mentioned above.


```python
def linearCost(X, y, theta):
    """
    INPUTS:
    -------
    X : array_like
        Inputs. Number of rows is m and number of columns is n+1 (incl. bias term).
        Shape (m, n+1)
    y : array_like
        Outputs at each data point x. 
        Shape (m, )
    theta : array_like
        Parameters for linear regression. 
        Shape (n+1, )
    OUTPUT:
    -------
    J : float
        The value of the cost function
    """
    # Get size of data and initialize the cost function J
    m = y.size
    J = 0
    
    # Use the vectorized implementation described above to get J
    J = (X @ theta - y).T @ (X @ theta - y) / (2*m)
    
    return J
```


```python
def gradientDescent(X, y, theta, alpha, num_iters):
    """
    INPUTS:
    -------
    X : array_like
        Inputs. Number of rows is m and number of columns is n+1 (incl. bias term).
        Shape (m, n+1)
    y : array_like
        Outputs at each data point x. 
        Shape (m, )
    theta : array_like
        Parameters for linear regression. 
        Shape (n+1, )
    alpha : float
        Learning rate
    num_iters : int
        Number of iterations to run gradient descent
    OUTPUT:
    -------
    theta : array_like
        The optimal parameters as found by gradient descent
    cost_history : array_like
        Array of with iterations in column 0 and cost function value at iteration i in column 1
        Shape (num_iters, 2)
    """
    # Find size of data set and initialize history list
    m = y.size
    cost_history = np.zeros([num_iters, 2])
    
    for i in range(num_iters):
        theta += (-1 * alpha) * X.T @ (X @ theta - y)
        cost_history[i, 0] = i
        cost_history[i, 1] = linearCost(X, y, theta)
    
    return theta, cost_history
```

With some hand tuning, a value of $$ \alpha $$ which does not diverge (but also does not converge) can be found:


```python
# Set theta, alpha, num iters, and initialize history
theta = np.array([0.,0.,0.])
alpha = 0.000000455
num_iters = 100
cost_history = np.zeros([num_iters, 2])

theta, cost_history = gradientDescent(X, y, theta, alpha, num_iters)

print("Value of theta: ", theta)

plt.figure(figsize=(14,10))
plt.scatter(cost_history[:,0],cost_history[:,1]) 
plt.xlabel("Number of iterations")
plt.ylabel("Value of cost function")
```

    Value of theta:  [0.00016822 0.00070681 0.007468  ]
    Text(0, 0.5, 'Value of cost function')




    
![png](\assets\img\langevin1\output_10_2.png)
    


Further efforts to tune $$ \alpha $$ do not result in convergence. The only method found to work so far has been standardization of the matrix $$ X $$. Getting convergence for the above model is not, however, the subject of interest for this article. We will rather ask a different question: can a Bayesian model outperform the above frequentist model above (without standardization)? Now, we introduce a Bayesian linear model and the Langevin algorithm.

## A Bayesian linear model
Let $$ \theta $$ have a flat prior. That is, $$ \theta \propto 1 $$. Also, let the likelihood $$ y | \theta \sim N(X\theta, \sigma^2 I_m). $$ Then the posterior is equal to the likelihood so that 

$$ P(y|\theta) \propto \exp\left[-\frac{1}{2\sigma^2}(y-X\theta)^T(y-X\theta)\right]. $$

Observe that 

$$ y^T[I_m - X(X^TX)^{-1}X^T]X(\theta-\hat{\theta}) = 0. $$

Then we obtain

$$ P(\theta|y) \propto \exp\left[-\frac{1}{2\sigma^2}(y-X\theta)^TX^TX(y-X\theta)\right] \sim N(\hat{\theta},\sigma^2(X^TX)^{-1}) $$.

## Langevin algorithm
The Langevin equation or differential equation is given by

$$ dB_t = \frac{1}{2}\nabla\log\pi(B_t)dt+dW_t, $$

where $$ W_t $$ is a multidimensional Wiener process. In this case, it can be shown that this has the stationary distribution $$ \pi(.) $$.

Consider $$ \pi(\theta) = P(\theta|y). $$ Then 

$$ \frac{1}{2}\nabla\log\pi(\theta)dt = -\frac{1}{2\sigma^2}X^T(X\theta-y). $$

Thus, we obtain

$$ \theta_{t+\Delta t} = \theta_t - \frac{\Delta t}{2\sigma^2} X^T(X\theta - y)+\varepsilon_t $$

where $$ \varepsilon_t \sim N(0, \Delta t I_{n+1}) $$.

The above says that we may be able to get a good sample from the target distribution (the posterior) by using a good choice for $$ \Delta t $$ and enough iterations. The random noise could prevent divergence from occuring, thus becoming an improvement over gradient descent.

### Variance estimate
In practice, $$ \sigma^2 $$ is not known, and so the estimate 

$$ \hat{\sigma}^2 = \frac{y^T[I_m - X(X^TX)^{-1}X^T]y}{m-n-1} $$

may be used.


```python
def langevin(X, y, theta, delta_t, num_iters, burn_in):
    """
    INPUTS:
    -------
    X : array_like
        Inputs. Number of rows is m and number of columns is n+1 (incl. bias term).
        Shape (m, n+1)
    y : array_like
        Outputs at each data point x. 
        Shape (m, )
    theta : array_like
        Parameters for linear regression. 
        Shape (n+1, )
    delta_t : float
        Time interval, directly related to alpha
    num_iters : int
        Number of iterations to run gradient descent
    burn_in : int
        Number of values to discard for the burn-in period
    OUTPUT:
    -------
    theta : array_like
        The optimal parameters as found by gradient descent
    cost_history : array_like
        Array of with iterations in column 0 and cost function value at iteration i in column 1
        Shape (num_iters, 2)
    theta_avg : array_like
        Array of average values for theta found using the Langevin algorithm
    """
    # Find size of data set and initialize history list
    m = y.size
    cost_history = np.zeros([num_iters, 2])
    n = X.shape[1] - 1
    theta_acc = np.zeros(theta.shape)
    
    # Use the estimate of variance to determine alpha
    var_est = (y.T @ (np.identity(m) - X @ np.linalg.inv(X.T @ X) @ X.T) @ y) / (m - n - 1)
    alpha = delta_t / (2 * var_est)
    
    for i in range(num_iters):
        theta -= alpha * X.T @ (X @ theta - y) + np.random.normal(0, np.sqrt(delta_t), (np.shape(X)[1], ))
        # theta -= alpha * X.T @ (X @ theta - y) + np.random.normal(0, 1, (np.shape(X)[1], ))
        #var = (y - X@theta).T @ (y - X@theta) / (m - n - 1)
        #alpha = 1 / (2 * var)
        cost_history[i, 0] = i
        cost_history[i, 1] = linearCost(X, y, theta)
        if num_iters >= burn_in:    
            theta_acc += theta
        
    theta_avg = theta_acc / burn_in
    
    return theta, cost_history, theta_avg
```


```python
# Set theta, delta_t, num iters, burn-in, and initialize history
theta = np.array([0.,0.,0.])
delta_t = 0.000000455
num_iters = 1000
burn_in = 100
cost_history = np.zeros([num_iters, 2])
theta_acc = np.zeros(theta.shape)

theta, cost_history, theta_avg = langevin(X, y, theta, delta_t, num_iters, burn_in)

print("Average value of theta: ", theta_avg)

print("Last value of theta: ", theta) # values from skl are about -0.1074, 1.0677, 0.0008

plt.figure(figsize=(14,10))
plt.scatter(cost_history[:,0],cost_history[:,1]) 
plt.xlabel("Number of iterations")
plt.ylabel("Value of cost function")
```

    Average value of theta:  [-8.75942972e+115 -6.29174773e+116 -1.00408784e+119]
    Last value of theta:  [-1.53530089e+118 -1.10278023e+119 -1.75990562e+121]
    Text(0, 0.5, 'Value of cost function')




    
![png](\assets\img\langevin1\output_14_2.png)
    


Unfortunately, this algorithm also is sensitive to the choice of $$ \alpha $$ (or in this case $$ \Delta t $$), despite adding in random noise. However, further tuning gives us some seemingly better results than gradient descent.


```python
# Set theta, delta_t, num iters, burn-in, and initialize history
theta = np.array([0.,0.,0.])
# finer time interval
delta_t = 0.0000000455
# more iterations
num_iters = 10000
burn_in = 1000
cost_history = np.zeros([num_iters, 2])
theta_acc = np.zeros(theta.shape)

theta, cost_history, theta_avg = langevin(X, y, theta, delta_t, num_iters, burn_in)

print("Average value of theta: ", theta_avg)

print("Last value of theta: ", theta) # values from skl are about -0.1074, 1.0677, 0.0008

plt.figure(figsize=(14,10))
plt.scatter(cost_history[:,0],cost_history[:,1]) 
plt.xlabel("Number of iterations")
plt.ylabel("Value of cost function")
```

    Average value of theta:  [-0.05940866  0.0822555   0.07426715]
    Last value of theta:  [-0.01973178  0.02512995  0.00711357]
    Text(0, 0.5, 'Value of cost function')




    
![png](\assets\img\langevin1\output_16_2.png)
    


These values at a glance do appear to be more reasonable than those found before.

## Conclusion
In this article, we proposed the Langevin algorithm as an alternative to gradient descent when the design matrix (or data) has large differences in magnitudes of eigenvalues. Although currently inconclusive, we will conduct further research in an attempt to compare the two methods. Thanks to Dr. Motoya Machida as the main contributor for the mathematics portion of this article.
